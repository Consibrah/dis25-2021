{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Wordnet\n",
    "(Syn-)Semantische Netze wie WordNet stellen wichtige Ressourcen für NLP dar. In diesem Tutorial werden sie grundlegende Funktionalitäten von WordNet und der deutschen Variante GermaNet anwenden. Nutzen Sie die Dokumentationen zu WordNet (https://www.nltk.org/howto/wordnet.html) und GermaNet (https://germanetpy.readthedocs.io/en/latest/). Sollten Sie mit der API Dokumentation nicht weiterkommen, nutzen Sie weitere Dokumentationen und Hilfestellungen wie stackoverflow.com oder Tutorials.\n",
    "\n",
    "## Aufgabe 1: Importieren der Module und Daten\n",
    "### a: Import von NLP Modulen\n",
    "Importieren Sie wie im ersten Tutorial Pandas, Numpy, NLTK und RE und importieren sie WordNet (als wn) von nltk.corpus.\n",
    "### b: Import von \"Quality-of-Life Modulen\"\n",
    "Oft sind Module nicht notwendig, erleichtern aber die Arbeit mit größeren Korpora. Installieren sie pandarallel. Importieren Sie wie im ersten Tutorial die Methode pandarallel (für Parallelization in Pandas) und initialisieren Sie diese mit pandarallel.initialize(). Sie können nun bei parallelisierbaren Aufgaben parallel\\_apply() anstelle der Pandas Methode apply() verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 1.0.5\n",
      "numpy 1.18.5\n",
      "nltk 3.5\n",
      "re 2.2.1\n",
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import pandas as pd\n",
    "print (\"pandas\", pd.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print (\"numpy\", np.__version__)\n",
    "\n",
    "import nltk\n",
    "print (\"nltk\", nltk.__version__)\n",
    "\n",
    "import re\n",
    "print (\"re\", re.__version__)\n",
    "\n",
    "from pandarallel import pandarallel  # parallelization\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Importieren der Daten\n",
    "Für das WordNet Tutorial ist ein Datensatz zu Biased Words (Wörtern die Voreingenommenheit oder unsachliche Wertung tragen) zur Verfügung gestellt. Importieren Sie das als pickle gespeicherte Dataframe \"data.pkl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>topic</th>\n",
       "      <th>Label_bias</th>\n",
       "      <th>biased_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YouTube is making clear there will be no “birt...</td>\n",
       "      <td>elections-2020</td>\n",
       "      <td>Biased</td>\n",
       "      <td>[belated, birtherism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The increasingly bitter dispute between Americ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>[bitter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So while there may be a humanitarian crisis dr...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>Biased</td>\n",
       "      <td>[crisis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A professor who teaches climate change classes...</td>\n",
       "      <td>environment</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>[legitimate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Looking around the United States, there is nev...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Biased</td>\n",
       "      <td>[killing, never, developing, humans, enough]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>In every case legislators are being swarmed by...</td>\n",
       "      <td>gender</td>\n",
       "      <td>Biased</td>\n",
       "      <td>[deceit, hysteria, swarmed, right-wing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>Polls show the transgender ideology is deeply ...</td>\n",
       "      <td>gender</td>\n",
       "      <td>Biased</td>\n",
       "      <td>[ideology, unpopular]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>Democrats and Republicans stood and applauded ...</td>\n",
       "      <td>gender</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>[saluted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>As a self-described Democratic socialist, Sen....</td>\n",
       "      <td>middle-class</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>[outspoken]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>During the segment, Colbert also bemoaned the ...</td>\n",
       "      <td>white-nationalism</td>\n",
       "      <td>Non-biased</td>\n",
       "      <td>[bemoaned]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence              topic  \\\n",
       "0     YouTube is making clear there will be no “birt...     elections-2020   \n",
       "1     The increasingly bitter dispute between Americ...              sport   \n",
       "2     So while there may be a humanitarian crisis dr...        immigration   \n",
       "3     A professor who teaches climate change classes...        environment   \n",
       "4     Looking around the United States, there is nev...           abortion   \n",
       "...                                                 ...                ...   \n",
       "1695  In every case legislators are being swarmed by...             gender   \n",
       "1696  Polls show the transgender ideology is deeply ...             gender   \n",
       "1697  Democrats and Republicans stood and applauded ...             gender   \n",
       "1698  As a self-described Democratic socialist, Sen....       middle-class   \n",
       "1699  During the segment, Colbert also bemoaned the ...  white-nationalism   \n",
       "\n",
       "      Label_bias                                  biased_words  \n",
       "0         Biased                         [belated, birtherism]  \n",
       "1     Non-biased                                      [bitter]  \n",
       "2         Biased                                      [crisis]  \n",
       "3     Non-biased                                  [legitimate]  \n",
       "4         Biased  [killing, never, developing, humans, enough]  \n",
       "...          ...                                           ...  \n",
       "1695      Biased       [deceit, hysteria, swarmed, right-wing]  \n",
       "1696      Biased                         [ideology, unpopular]  \n",
       "1697  Non-biased                                     [saluted]  \n",
       "1698  Non-biased                                   [outspoken]  \n",
       "1699  Non-biased                                    [bemoaned]  \n",
       "\n",
       "[1700 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Synsets\n",
    "Synsets sind die bedeutungsunterscheidbaren Definitionen von Wörtern bzw. Tokens. Ein Anwendungszweck ist es, den Kontext zu Wörtern um bedeutungsgleiche Wörter zu erweitern. Für den gegebenen Datensatz möchten wir die Synonyme zu den Biased Words in Form der \\textit{Lemmas} zu den zugehörigen Synsets erfassen.\n",
    "### a: Synsets finden\n",
    "Extrahieren Sie eine Liste der paarweise verschiedenen biased words (Spalte \"Label_bias\") aus dem Datensatz und speichern Sie diese in einer separaten Liste \"b_words\" (nicht im Dataframe!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_words = list(set([a for b in data.biased_words.tolist() for a in b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b: Synonyme finden\n",
    "Für jedes der Wörter, bestimmen sie alle Synsets und alle zu den Synsets gehörigen Lemmas. Speichern Sie alle paarweise verschiedenen Lemmas zu allen biased Words in einer Liste \"b_words_synonyms\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "for word in b_words:\n",
    "    for syn in wn.synsets(word):\n",
    "         for lemma in syn.lemmas():\n",
    "                synonyms.extend([lemma.name()])\n",
    "\n",
    "b_words_synonyms = list(set(synonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Kandidaten für weitere biased Words\n",
    "Die soeben bestimmten Synonyme zu den Biased Words stellen einen guten Startpunkt für die manuelle Bestimmung von weiteren Biased Words dar. Erstellen Sie abschließend eine Liste \"new_b_words\", in der Sie alle Wörter der Liste \"b_words_synonyms\" speichern, die nicht in der ursprünglichen Liste von \"b_words\" enthalten sind. Lassen Sie sich für alle 3 erzeugten Listen die Anzahl der enthaltenen Wörter ausgeben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_b_words = [a for a in b_words_synonyms if a not in b_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2258\n",
      "10067\n",
      "8512\n"
     ]
    }
   ],
   "source": [
    "print(len(b_words))\n",
    "print(len(b_words_synonyms))\n",
    "print(len(new_b_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 2 GermaNet\n",
    "GermaNet ist die an der Uni Tübingen entwickelte Version von Germanet für ralationale synsemantische Wortnetze in deutscher Sprache. Obwohl die grundlegenden Funktionalitäten weitestgehend identisch zu Wordnet sind, werden sie anders aufgerufen. Da Sie sich im weiteren Verlauf dieser Veranstaltung vermehrt mit den Datensätzen aus dem ESUPOL Projekt beschäftigen werden, könnte GermaNet ein sinnvolles Tool für semantische Analysen darstellen.\n",
    "\n",
    "### a: Germanet installieren und importieren\n",
    "GermaNet ist nicht öffentlich zugänglich, die TH Köln hat eine Lizenz für die Verwendung im Rahmen von Lehre und Forschung zur Verfügung gestellt bekommen. Nutzen Sie GermaNet daher nur für Aufgaben und Projekte im Rahmen dieser Lehrveranstaltung. \n",
    "Kopieren Sie den bereitgestellen Ordner \"germanetpy\" in ihren site-packages Ordner. \n",
    "Vergewissern Sie sich, dass Germanet ordnungsgemäß gefunden wird. Sie können sicher gehen und das Modul noch einmal über pip installieren:\n",
    "```python\n",
    "import sys\n",
    "!{sys.executable} -m pip install -U germanetpy\n",
    "```\n",
    "\n",
    "WordNet nutzt XML für die Relationen und Textdateien für Frequencies, die an einem bestimmten Ort abgelegt sein müssen. Folgen sie den Vorgaben der offiziellen API um Germanet richtig einzurichten:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from germanetpy.germanet import Germanet\n",
    "\n",
    "data_path = str(Path.home()) + \"/germanet/GN_V150/GN_V150_XML\"\n",
    "frequencylist_nouns = str(Path.home()) + \"/germanet/GN_V150/FreqLists/noun_freqs_decow14_16.txt\"\n",
    "germanet = Germanet(data_path)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load GermaNet data...: 100%|█████████▉| 99.99999999999996/100 [00:10<00:00,  9.73it/s] \n",
      "Load Wictionary data...: 100%|██████████| 100.0/100 [00:00<00:00, 573.59it/s]            \n",
      "Load Ili records...: 100%|██████████| 100.0/100 [00:00<00:00, 198970.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from germanetpy.germanet import Germanet\n",
    "\n",
    "data_path = str(Path.home()) + \"/germanet/GN_V150/GN_V150_XML\"\n",
    "frequencylist_nouns = str(Path.home()) + \"/germanet/GN_V150/FreqLists/noun_freqs_decow14_16.txt\"\n",
    "germanet = Germanet(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b: Datensatz importieren\n",
    "Importieren sie die Datei \"single_term_suggestions.txt\" als Dataframe. Die Datei enthält eine Liste von single-Word Query Suggestions aus dem in der Vorlesung vorgestellten Datensatz zur Bundestagswahl 2017. Sie können die Pandas-Methode \"read_csv\" nutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger = pd.read_csv(\"single_term_suggestions.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4: Germanet nutzen\n",
    "### a: Synsets\n",
    "Bestimmen Sie jeweils für alle Suggestions (also jede Zeile der Daten) alle Synsets und speichern Sie die Liste in einer separaten Spalte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger[\"synsets_ger\"] = data_ger.apply(lambda row: germanet.get_synsets_by_orthform(row[\"suggestion_ger\"], ignorecase = True), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b: Lexikalische Einheiten\n",
    "Bestimmen sie für jede Zeile für alle Synsets jeweils alle Lemmas (lexikalischen Einheiten, also lexunits) und tragen sie diese als eine Liste in eine neue Spalte ein. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_synsets(syns):\n",
    "    ret = []\n",
    "    try:\n",
    "        for syn in syns:\n",
    "            for lemma in syn.lexunits:\n",
    "                ret.extend(lemma.get_all_orthforms())\n",
    "    #print(ret)\n",
    "        return ret\n",
    "    except:\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger[\"lexunits\"] = data_ger.apply(lambda row: get_names_synsets(row[\"synsets_ger\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c: Hypernyms\n",
    "Semantische Netze wie Germanet beschreiben Ist-Beziehungen zwischen Synsets. Hypernyme (Übertypen) und Hyponyme (Untertypen) können hilfreich für die Klassifizierung von Begriffen sein.\n",
    "Bestimmen Sie für alle Synsets jeder Suggestion jeweils alle Hypernyms und speichern sie deren Synsets in einer Spalte \"hypernyms\". Bestimmen sie anschließend die Lemmas dieser Hypernyme und speichern Sie diese in einer separaten Spalte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms_from_list_of_synsets(list_syns):\n",
    "    for syn in list_syns:\n",
    "        return syn.direct_hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger[\"hypernyms\"] = data_ger.apply(lambda row: get_hypernyms_from_list_of_synsets(row[\"synsets_ger\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger[\"lexunits_hypernyms\"] = data_ger.apply(lambda row: get_names_synsets(row[\"hypernyms\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d: Hyponyms\n",
    "Gehen sie wie in c vor, nur bestimmen Sie dieses Mal die Hyponyme der Suggestions und deren Lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms_from_list_of_synsets(list_syns):\n",
    "    for syn in list_syns:\n",
    "        return syn.direct_hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger[\"hyponyms\"] = data_ger.apply(lambda row: get_hyponyms_from_list_of_synsets(row[\"synsets_ger\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger[\"lexunits_hyponyms\"] = data_ger.apply(lambda row: get_names_synsets(row[\"hyponyms\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suggestion_ger</th>\n",
       "      <th>lexunits_hypernyms</th>\n",
       "      <th>lexunits_hyponyms</th>\n",
       "      <th>lexunits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>[Fäkalien]</td>\n",
       "      <td>[Mist, Dung, Klabusterbeere, Kinderkacke, Losu...</td>\n",
       "      <td>[Kot, Scheiße, Exkrement, Stuhl, Kacke, Kaka, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aach</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aalten</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aarburg</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaronn</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>zwangsdienst</td>\n",
       "      <td>[Dienst]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Zwangsdienst]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>zwangshypothek</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>zweibruecken</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>zwickau</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Zwickau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>zwillinge</td>\n",
       "      <td>[Sternzeichen, Sternbild, Tierkreiszeichen, Haus]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Zwillinge]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3762 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      suggestion_ger                                 lexunits_hypernyms  \\\n",
       "0                 aa                                         [Fäkalien]   \n",
       "1               aach                                                 []   \n",
       "2             aalten                                                 []   \n",
       "3            aarburg                                                 []   \n",
       "4             aaronn                                                 []   \n",
       "...              ...                                                ...   \n",
       "3757    zwangsdienst                                           [Dienst]   \n",
       "3758  zwangshypothek                                                 []   \n",
       "3759    zweibruecken                                                 []   \n",
       "3760         zwickau                                            [Stadt]   \n",
       "3761       zwillinge  [Sternzeichen, Sternbild, Tierkreiszeichen, Haus]   \n",
       "\n",
       "                                      lexunits_hyponyms  \\\n",
       "0     [Mist, Dung, Klabusterbeere, Kinderkacke, Losu...   \n",
       "1                                                    []   \n",
       "2                                                    []   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "3757                                                 []   \n",
       "3758                                                 []   \n",
       "3759                                                 []   \n",
       "3760                                                 []   \n",
       "3761                                                 []   \n",
       "\n",
       "                                               lexunits  \n",
       "0     [Kot, Scheiße, Exkrement, Stuhl, Kacke, Kaka, ...  \n",
       "1                                                    []  \n",
       "2                                                    []  \n",
       "3                                                    []  \n",
       "4                                                    []  \n",
       "...                                                 ...  \n",
       "3757                                     [Zwangsdienst]  \n",
       "3758                                                 []  \n",
       "3759                                                 []  \n",
       "3760                                          [Zwickau]  \n",
       "3761                                        [Zwillinge]  \n",
       "\n",
       "[3762 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ger[[\"suggestion_ger\",\"lexunits_hypernyms\", \"lexunits_hyponyms\", \"lexunits\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lassen Sie sich die Anzahl aller paarweise verschiedenen Hypernyme, Hyponyme und Lemmas ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexunits_hypernyms = list(set([a for b in data_ger.lexunits_hypernyms for a in b]))\n",
    "lexunits_hyponyms = list(set([a for b in data_ger.lexunits_hyponyms for a in b]))\n",
    "lexunits = list(set([a for b in data_ger.lexunits for a in b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "12417\n"
     ]
    }
   ],
   "source": [
    "print(len(lexunits_hypernyms))\n",
    "print(len(lexunits_hyponyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e: Klassifizierungs-Tags mit Synsets\n",
    "Synsemantische Netze können beispielsweise genutzt werden, um Begriffe zu klassifizieren. \n",
    "Nutzen Sie die identifizierten Hypernyme, um alle Städte in dem Dataset zu finden. Klassifizieren Sie in einer Spalte \"location\" alle Städte, Länder und Orte als \"True\" und alle anderen Suggestions als \"False\". Lassen sie sich ein Sub-Dataframe aller Locations im Datensatz ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_in_list(liste, words):\n",
    "    for word in words:\n",
    "        if word in liste:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger[\"city\"] = data_ger.apply(lambda row: is_word_in_list(row[\"lexunits_hypernyms\"], [\"Land\", \"Dorf\", \"Stadt\", \"Ort\", \"Platz\", \"Staat\", \"Bundesland\" ]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suggestion_ger</th>\n",
       "      <th>synsets_ger</th>\n",
       "      <th>lexunits</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>lexunits_hypernyms</th>\n",
       "      <th>hyponyms</th>\n",
       "      <th>lexunits_hyponyms</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>afghanistan</td>\n",
       "      <td>[Synset(id=s44583, lexunits=Afghanistan)]</td>\n",
       "      <td>[Afghanistan]</td>\n",
       "      <td>{Synset(id=s44177, lexunits=Land, Staat)}</td>\n",
       "      <td>[Land, Staat]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>albanien</td>\n",
       "      <td>[Synset(id=s44497, lexunits=Albanien)]</td>\n",
       "      <td>[Albanien]</td>\n",
       "      <td>{Synset(id=s44177, lexunits=Land, Staat)}</td>\n",
       "      <td>[Land, Staat]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>aleppo</td>\n",
       "      <td>[Synset(id=s73659, lexunits=Aleppo)]</td>\n",
       "      <td>[Aleppo]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>altenburg</td>\n",
       "      <td>[Synset(id=s73693, lexunits=Altenburg)]</td>\n",
       "      <td>[Altenburg]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>amorbach</td>\n",
       "      <td>[Synset(id=s44036, lexunits=Amorbach)]</td>\n",
       "      <td>[Amorbach]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3666</th>\n",
       "      <td>wittenberg</td>\n",
       "      <td>[Synset(id=s44111, lexunits=Wittenberg)]</td>\n",
       "      <td>[Wittenberg]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>worms</td>\n",
       "      <td>[Synset(id=s44019, lexunits=Worms)]</td>\n",
       "      <td>[Worms]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3702</th>\n",
       "      <td>wuppertal</td>\n",
       "      <td>[Synset(id=s44061, lexunits=Wuppertal)]</td>\n",
       "      <td>[Wuppertal]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3707</th>\n",
       "      <td>xanten</td>\n",
       "      <td>[Synset(id=s44082, lexunits=Xanten)]</td>\n",
       "      <td>[Xanten]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>zwickau</td>\n",
       "      <td>[Synset(id=s44112, lexunits=Zwickau)]</td>\n",
       "      <td>[Zwickau]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     suggestion_ger                                synsets_ger       lexunits  \\\n",
       "43      afghanistan  [Synset(id=s44583, lexunits=Afghanistan)]  [Afghanistan]   \n",
       "64         albanien     [Synset(id=s44497, lexunits=Albanien)]     [Albanien]   \n",
       "69           aleppo       [Synset(id=s73659, lexunits=Aleppo)]       [Aleppo]   \n",
       "99        altenburg    [Synset(id=s73693, lexunits=Altenburg)]    [Altenburg]   \n",
       "121        amorbach     [Synset(id=s44036, lexunits=Amorbach)]     [Amorbach]   \n",
       "...             ...                                        ...            ...   \n",
       "3666     wittenberg   [Synset(id=s44111, lexunits=Wittenberg)]   [Wittenberg]   \n",
       "3691          worms        [Synset(id=s44019, lexunits=Worms)]        [Worms]   \n",
       "3702      wuppertal    [Synset(id=s44061, lexunits=Wuppertal)]    [Wuppertal]   \n",
       "3707         xanten       [Synset(id=s44082, lexunits=Xanten)]       [Xanten]   \n",
       "3760        zwickau      [Synset(id=s44112, lexunits=Zwickau)]      [Zwickau]   \n",
       "\n",
       "                                      hypernyms lexunits_hypernyms hyponyms  \\\n",
       "43    {Synset(id=s44177, lexunits=Land, Staat)}      [Land, Staat]       {}   \n",
       "64    {Synset(id=s44177, lexunits=Land, Staat)}      [Land, Staat]       {}   \n",
       "69          {Synset(id=s43645, lexunits=Stadt)}            [Stadt]       {}   \n",
       "99          {Synset(id=s43645, lexunits=Stadt)}            [Stadt]       {}   \n",
       "121         {Synset(id=s43645, lexunits=Stadt)}            [Stadt]       {}   \n",
       "...                                         ...                ...      ...   \n",
       "3666        {Synset(id=s43645, lexunits=Stadt)}            [Stadt]       {}   \n",
       "3691        {Synset(id=s43645, lexunits=Stadt)}            [Stadt]       {}   \n",
       "3702        {Synset(id=s43645, lexunits=Stadt)}            [Stadt]       {}   \n",
       "3707        {Synset(id=s43645, lexunits=Stadt)}            [Stadt]       {}   \n",
       "3760        {Synset(id=s43645, lexunits=Stadt)}            [Stadt]       {}   \n",
       "\n",
       "     lexunits_hyponyms  city  \n",
       "43                  []  True  \n",
       "64                  []  True  \n",
       "69                  []  True  \n",
       "99                  []  True  \n",
       "121                 []  True  \n",
       "...                ...   ...  \n",
       "3666                []  True  \n",
       "3691                []  True  \n",
       "3702                []  True  \n",
       "3707                []  True  \n",
       "3760                []  True  \n",
       "\n",
       "[194 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ger.loc[(data_ger[\"city\"]==True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUSAUFGABE: Semantische Ähnlichkeit und Verwandschaft\n",
    "Über die relationale Struktur von Synsets lässt sich die Ähnlichkeit bzw. Verwandtschaft zweier Begriffe gleicher Wortart ableiten. Die Ähnlichkeit kann wiederum zum Beispiel zur Beseitigung von Ambiguität verwendet werden. Im Fall des Datensatzes bilden die Terme Suchvorschläge zu personenbezogenen Suchen in Suchmaschienen, denen die Namen von Politikern als Suchterm zugrunde liegen. Der Term \"Abbruch\" wurde also als Suchvorschlag für mindestens einen Namen eines Politikers vorgeschlagen. Um von den Suggestions nun jeweils das relevante Synset zu identifizieren, kann die Ähnlichkeit zum Synset \"Politiker\" ( Synset(id=s34818, lexunits=Politiker, Politikerin) ) bestimmt werden. \n",
    "\n",
    "Gehen Sie wie im offiziellen Tutorial zu GermaNet (https://github.com/Germanet-sfs/germanetTutorials/tree/master/pythonAPI}) erläutert vor, um für alle Synsets aller Suggestions jeweils die Similarity zum Politiker Synset zu berechnen und speichern Sie das Synsets mit der höchsten Ähnlichkeit in einer Spalte \"best_syn\". Verwenden Sie entweder Path- oder IC- basierte Ähnlichkeit oder führen Sie beides separat durch. Exportieren Sie abschließend ein Sub-Dataframe, in dem nur solche Zeilen enthalten sind, deren Suggestion über mindestens 2 Synsets verfügt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ger.at[1, \"synsets_ger\"]==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ger[\"synsets_check\"] = data_ger.apply(lambda row: len(row[\"synsets_ger\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suggestion_ger</th>\n",
       "      <th>synsets_ger</th>\n",
       "      <th>lexunits</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>lexunits_hypernyms</th>\n",
       "      <th>hyponyms</th>\n",
       "      <th>lexunits_hyponyms</th>\n",
       "      <th>city</th>\n",
       "      <th>synsets_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>[Synset(id=s26358, lexunits=Kot, Scheiße, Exkr...</td>\n",
       "      <td>[Kot, Scheiße, Exkrement, Stuhl, Kacke, Kaka, ...</td>\n",
       "      <td>{Synset(id=s26356, lexunits=Fäkalien)}</td>\n",
       "      <td>[Fäkalien]</td>\n",
       "      <td>{Synset(id=s26359, lexunits=Mist, Dung), Synse...</td>\n",
       "      <td>[Mist, Dung, Klabusterbeere, Kinderkacke, Losu...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aach</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aalten</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aarburg</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaronn</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>zwangsdienst</td>\n",
       "      <td>[Synset(id=s131522, lexunits=Zwangsdienst)]</td>\n",
       "      <td>[Zwangsdienst]</td>\n",
       "      <td>{Synset(id=s19797, lexunits=Dienst)}</td>\n",
       "      <td>[Dienst]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>zwangshypothek</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>zweibruecken</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>zwickau</td>\n",
       "      <td>[Synset(id=s44112, lexunits=Zwickau)]</td>\n",
       "      <td>[Zwickau]</td>\n",
       "      <td>{Synset(id=s43645, lexunits=Stadt)}</td>\n",
       "      <td>[Stadt]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>zwillinge</td>\n",
       "      <td>[Synset(id=s29525, lexunits=Zwillinge)]</td>\n",
       "      <td>[Zwillinge]</td>\n",
       "      <td>{Synset(id=s29522, lexunits=Sternzeichen, Ster...</td>\n",
       "      <td>[Sternzeichen, Sternbild, Tierkreiszeichen, Haus]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3762 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      suggestion_ger                                        synsets_ger  \\\n",
       "0                 aa  [Synset(id=s26358, lexunits=Kot, Scheiße, Exkr...   \n",
       "1               aach                                                 []   \n",
       "2             aalten                                                 []   \n",
       "3            aarburg                                                 []   \n",
       "4             aaronn                                                 []   \n",
       "...              ...                                                ...   \n",
       "3757    zwangsdienst        [Synset(id=s131522, lexunits=Zwangsdienst)]   \n",
       "3758  zwangshypothek                                                 []   \n",
       "3759    zweibruecken                                                 []   \n",
       "3760         zwickau              [Synset(id=s44112, lexunits=Zwickau)]   \n",
       "3761       zwillinge            [Synset(id=s29525, lexunits=Zwillinge)]   \n",
       "\n",
       "                                               lexunits  \\\n",
       "0     [Kot, Scheiße, Exkrement, Stuhl, Kacke, Kaka, ...   \n",
       "1                                                    []   \n",
       "2                                                    []   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "3757                                     [Zwangsdienst]   \n",
       "3758                                                 []   \n",
       "3759                                                 []   \n",
       "3760                                          [Zwickau]   \n",
       "3761                                        [Zwillinge]   \n",
       "\n",
       "                                              hypernyms  \\\n",
       "0                {Synset(id=s26356, lexunits=Fäkalien)}   \n",
       "1                                                  None   \n",
       "2                                                  None   \n",
       "3                                                  None   \n",
       "4                                                  None   \n",
       "...                                                 ...   \n",
       "3757               {Synset(id=s19797, lexunits=Dienst)}   \n",
       "3758                                               None   \n",
       "3759                                               None   \n",
       "3760                {Synset(id=s43645, lexunits=Stadt)}   \n",
       "3761  {Synset(id=s29522, lexunits=Sternzeichen, Ster...   \n",
       "\n",
       "                                     lexunits_hypernyms  \\\n",
       "0                                            [Fäkalien]   \n",
       "1                                                    []   \n",
       "2                                                    []   \n",
       "3                                                    []   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "3757                                           [Dienst]   \n",
       "3758                                                 []   \n",
       "3759                                                 []   \n",
       "3760                                            [Stadt]   \n",
       "3761  [Sternzeichen, Sternbild, Tierkreiszeichen, Haus]   \n",
       "\n",
       "                                               hyponyms  \\\n",
       "0     {Synset(id=s26359, lexunits=Mist, Dung), Synse...   \n",
       "1                                                  None   \n",
       "2                                                  None   \n",
       "3                                                  None   \n",
       "4                                                  None   \n",
       "...                                                 ...   \n",
       "3757                                                 {}   \n",
       "3758                                               None   \n",
       "3759                                               None   \n",
       "3760                                                 {}   \n",
       "3761                                                 {}   \n",
       "\n",
       "                                      lexunits_hyponyms   city  synsets_check  \n",
       "0     [Mist, Dung, Klabusterbeere, Kinderkacke, Losu...  False              2  \n",
       "1                                                    []  False              0  \n",
       "2                                                    []  False              0  \n",
       "3                                                    []  False              0  \n",
       "4                                                    []  False              0  \n",
       "...                                                 ...    ...            ...  \n",
       "3757                                                 []  False              1  \n",
       "3758                                                 []  False              0  \n",
       "3759                                                 []  False              0  \n",
       "3760                                                 []   True              1  \n",
       "3761                                                 []  False              1  \n",
       "\n",
       "[3762 rows x 9 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bonus = data_ger.loc[(data_ger[\"synsets_check\"]>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suggestion_ger</th>\n",
       "      <th>synsets_ger</th>\n",
       "      <th>lexunits</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>lexunits_hypernyms</th>\n",
       "      <th>hyponyms</th>\n",
       "      <th>lexunits_hyponyms</th>\n",
       "      <th>city</th>\n",
       "      <th>synsets_check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>[Synset(id=s26358, lexunits=Kot, Scheiße, Exkr...</td>\n",
       "      <td>[Kot, Scheiße, Exkrement, Stuhl, Kacke, Kaka, ...</td>\n",
       "      <td>{Synset(id=s26356, lexunits=Fäkalien)}</td>\n",
       "      <td>[Fäkalien]</td>\n",
       "      <td>{Synset(id=s26359, lexunits=Mist, Dung), Synse...</td>\n",
       "      <td>[Mist, Dung, Klabusterbeere, Kinderkacke, Losu...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abbruch</td>\n",
       "      <td>[Synset(id=s106285, lexunits=Abbruch), Synset(...</td>\n",
       "      <td>[Abbruch, Abbruch, Beendigung, Beenden, Aufhör...</td>\n",
       "      <td>{Synset(id=s17129, lexunits=Schaden, Schädigung)}</td>\n",
       "      <td>[Schaden, Schädigung]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abnehmen</td>\n",
       "      <td>[Synset(id=s52508, lexunits=wegnehmen, abnehme...</td>\n",
       "      <td>[wegnehmen, abnehmen, fortnehmen, abchecken, a...</td>\n",
       "      <td>{Synset(id=s52497, lexunits=nehmen)}</td>\n",
       "      <td>[nehmen]</td>\n",
       "      <td>{Synset(id=s97237, lexunits=rupfen), Synset(id...</td>\n",
       "      <td>[rupfen, entrechten, entziehen, abknöpfen, ent...</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>abschied</td>\n",
       "      <td>[Synset(id=s17481, lexunits=Abschied, Lebewohl...</td>\n",
       "      <td>[Abschied, Lebewohl, Verabschiedung, Abschied]</td>\n",
       "      <td>{Synset(id=s17478, lexunits=Trennung)}</td>\n",
       "      <td>[Trennung]</td>\n",
       "      <td>{Synset(id=s123668, lexunits=Bühnenabschied)}</td>\n",
       "      <td>[Bühnenabschied]</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>adel</td>\n",
       "      <td>[Synset(id=s32247, lexunits=Adelstitel, Adel, ...</td>\n",
       "      <td>[Adelstitel, Adel, Adelsbezeichnung, Adelsgesc...</td>\n",
       "      <td>{Synset(id=s32215, lexunits=Titel)}</td>\n",
       "      <td>[Titel]</td>\n",
       "      <td>{Synset(id=s61467, lexunits=Kaiserinmutter), S...</td>\n",
       "      <td>[Kaiserinmutter, Marchese, Doge, Raugraf, Baro...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>zirkus</td>\n",
       "      <td>[Synset(id=s108415, lexunits=Zirkus), Synset(i...</td>\n",
       "      <td>[Circus, Zirkus, Circus, Zirkus, Zirkusunterne...</td>\n",
       "      <td>{Synset(id=s42745, lexunits=Arena, Stadion)}</td>\n",
       "      <td>[Arena, Stadion]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>zitat</td>\n",
       "      <td>[Synset(id=s32274, lexunits=Zitat), Synset(id=...</td>\n",
       "      <td>[Zitat, Zitat]</td>\n",
       "      <td>{Synset(id=s32267, lexunits=Redewendung, Idiom...</td>\n",
       "      <td>[Redewendung, Idiom, Redensart, Wendung]</td>\n",
       "      <td>{Synset(id=s137738, lexunits=Selbstzitat)}</td>\n",
       "      <td>[Selbstzitat]</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>zug</td>\n",
       "      <td>[Synset(id=s76189, lexunits=Zug), Synset(id=s4...</td>\n",
       "      <td>[Zug, Luftzug, Zug, Luft, Gesichtszug, Zug, Zu...</td>\n",
       "      <td>{Synset(id=s24150, lexunits=Formation, Gruppie...</td>\n",
       "      <td>[Formation, Gruppierung]</td>\n",
       "      <td>{Synset(id=s133640, lexunits=Einsatzzug), Syns...</td>\n",
       "      <td>[Einsatzzug, Fernmeldezug]</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>zukunft</td>\n",
       "      <td>[Synset(id=s51054, lexunits=Zukunft, Vorzeitig...</td>\n",
       "      <td>[Zukunft, Vorzeitigkeit, Hinkunft, Futur, Zuku...</td>\n",
       "      <td>{Synset(id=s51051, lexunits=Zeitstufe)}</td>\n",
       "      <td>[Zeitstufe]</td>\n",
       "      <td>{Synset(id=s22920, lexunits=Nachwelt), Synset(...</td>\n",
       "      <td>[Nachwelt, Vorweg, Energiezukunft]</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>zusammenbruch</td>\n",
       "      <td>[Synset(id=s17538, lexunits=Kollaps, Zusammenb...</td>\n",
       "      <td>[Kollaps, Zusammenbruch, Zusammenbruch, Crash]</td>\n",
       "      <td>{Synset(id=s17096, lexunits=Vorfall, Begebenhe...</td>\n",
       "      <td>[Vorfall, Begebenheit, Begebnis, Vorkommnis]</td>\n",
       "      <td>{Synset(id=s17539, lexunits=Verkehrskollaps)}</td>\n",
       "      <td>[Verkehrskollaps]</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>451 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     suggestion_ger                                        synsets_ger  \\\n",
       "0                aa  [Synset(id=s26358, lexunits=Kot, Scheiße, Exkr...   \n",
       "6           abbruch  [Synset(id=s106285, lexunits=Abbruch), Synset(...   \n",
       "15         abnehmen  [Synset(id=s52508, lexunits=wegnehmen, abnehme...   \n",
       "18         abschied  [Synset(id=s17481, lexunits=Abschied, Lebewohl...   \n",
       "26             adel  [Synset(id=s32247, lexunits=Adelstitel, Adel, ...   \n",
       "...             ...                                                ...   \n",
       "3741         zirkus  [Synset(id=s108415, lexunits=Zirkus), Synset(i...   \n",
       "3742          zitat  [Synset(id=s32274, lexunits=Zitat), Synset(id=...   \n",
       "3752            zug  [Synset(id=s76189, lexunits=Zug), Synset(id=s4...   \n",
       "3753        zukunft  [Synset(id=s51054, lexunits=Zukunft, Vorzeitig...   \n",
       "3755  zusammenbruch  [Synset(id=s17538, lexunits=Kollaps, Zusammenb...   \n",
       "\n",
       "                                               lexunits  \\\n",
       "0     [Kot, Scheiße, Exkrement, Stuhl, Kacke, Kaka, ...   \n",
       "6     [Abbruch, Abbruch, Beendigung, Beenden, Aufhör...   \n",
       "15    [wegnehmen, abnehmen, fortnehmen, abchecken, a...   \n",
       "18       [Abschied, Lebewohl, Verabschiedung, Abschied]   \n",
       "26    [Adelstitel, Adel, Adelsbezeichnung, Adelsgesc...   \n",
       "...                                                 ...   \n",
       "3741  [Circus, Zirkus, Circus, Zirkus, Zirkusunterne...   \n",
       "3742                                     [Zitat, Zitat]   \n",
       "3752  [Zug, Luftzug, Zug, Luft, Gesichtszug, Zug, Zu...   \n",
       "3753  [Zukunft, Vorzeitigkeit, Hinkunft, Futur, Zuku...   \n",
       "3755     [Kollaps, Zusammenbruch, Zusammenbruch, Crash]   \n",
       "\n",
       "                                              hypernyms  \\\n",
       "0                {Synset(id=s26356, lexunits=Fäkalien)}   \n",
       "6     {Synset(id=s17129, lexunits=Schaden, Schädigung)}   \n",
       "15                 {Synset(id=s52497, lexunits=nehmen)}   \n",
       "18               {Synset(id=s17478, lexunits=Trennung)}   \n",
       "26                  {Synset(id=s32215, lexunits=Titel)}   \n",
       "...                                                 ...   \n",
       "3741       {Synset(id=s42745, lexunits=Arena, Stadion)}   \n",
       "3742  {Synset(id=s32267, lexunits=Redewendung, Idiom...   \n",
       "3752  {Synset(id=s24150, lexunits=Formation, Gruppie...   \n",
       "3753            {Synset(id=s51051, lexunits=Zeitstufe)}   \n",
       "3755  {Synset(id=s17096, lexunits=Vorfall, Begebenhe...   \n",
       "\n",
       "                                lexunits_hypernyms  \\\n",
       "0                                       [Fäkalien]   \n",
       "6                            [Schaden, Schädigung]   \n",
       "15                                        [nehmen]   \n",
       "18                                      [Trennung]   \n",
       "26                                         [Titel]   \n",
       "...                                            ...   \n",
       "3741                              [Arena, Stadion]   \n",
       "3742      [Redewendung, Idiom, Redensart, Wendung]   \n",
       "3752                      [Formation, Gruppierung]   \n",
       "3753                                   [Zeitstufe]   \n",
       "3755  [Vorfall, Begebenheit, Begebnis, Vorkommnis]   \n",
       "\n",
       "                                               hyponyms  \\\n",
       "0     {Synset(id=s26359, lexunits=Mist, Dung), Synse...   \n",
       "6                                                    {}   \n",
       "15    {Synset(id=s97237, lexunits=rupfen), Synset(id...   \n",
       "18        {Synset(id=s123668, lexunits=Bühnenabschied)}   \n",
       "26    {Synset(id=s61467, lexunits=Kaiserinmutter), S...   \n",
       "...                                                 ...   \n",
       "3741                                                 {}   \n",
       "3742         {Synset(id=s137738, lexunits=Selbstzitat)}   \n",
       "3752  {Synset(id=s133640, lexunits=Einsatzzug), Syns...   \n",
       "3753  {Synset(id=s22920, lexunits=Nachwelt), Synset(...   \n",
       "3755      {Synset(id=s17539, lexunits=Verkehrskollaps)}   \n",
       "\n",
       "                                      lexunits_hyponyms   city  synsets_check  \n",
       "0     [Mist, Dung, Klabusterbeere, Kinderkacke, Losu...  False              2  \n",
       "6                                                    []  False              5  \n",
       "15    [rupfen, entrechten, entziehen, abknöpfen, ent...  False             10  \n",
       "18                                     [Bühnenabschied]  False              2  \n",
       "26    [Kaiserinmutter, Marchese, Doge, Raugraf, Baro...  False              2  \n",
       "...                                                 ...    ...            ...  \n",
       "3741                                                 []  False              4  \n",
       "3742                                      [Selbstzitat]  False              2  \n",
       "3752                         [Einsatzzug, Fernmeldezug]  False             14  \n",
       "3753                 [Nachwelt, Vorweg, Energiezukunft]  False              2  \n",
       "3755                                  [Verkehrskollaps]  False              2  \n",
       "\n",
       "[451 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file /home/fabian/germanet/GN_V150/FreqLists/noun_freqs_decow14_16.txt does not exist\n"
     ]
    }
   ],
   "source": [
    "from germanetpy.path_based_relatedness_measures import PathBasedRelatedness\n",
    "from germanetpy.synset import WordCategory\n",
    "from germanetpy.icbased_similarity import ICBasedSimilarity\n",
    "\n",
    "relatedness_nouns = ICBasedSimilarity(germanet=germanet, \n",
    "                                      wordcategory=WordCategory.nomen,\n",
    "                                      path=frequencylist_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset(id=s34818, lexunits=Politiker, Politikerin)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_politikerIn = germanet.get_synsets_by_orthform(\"Politiker\")\n",
    "syn_politikerIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_distance_to_politician(synset):\n",
    "    syn_p = germanet.get_synsets_by_orthform(\"Politiker\").pop()\n",
    "    path_distance = synset.shortest_path_distance(syn_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_related_to_politician_path(list_syns):\n",
    "    # First, construct a path-based similarity object. \n",
    "    # The johannis_wurm and leber_trans synsets are maximally far apart among nouns:\n",
    "    johannis_wurm = germanet.get_synset_by_id(\"s49774\")\n",
    "    leber_trans = germanet.get_synset_by_id(\"s83979\")\n",
    "    relatedness_calculator = PathBasedRelatedness(germanet=germanet, category=WordCategory.nomen, max_len=35, max_depth=20, synset_pair=(johannis_wurm, leber_trans))\n",
    "    \n",
    "    syn_p = germanet.get_synsets_by_orthform(\"Politiker\").pop()\n",
    "    \n",
    "    res = {}\n",
    "    for syn in list_syns:\n",
    "        if syn.word_category == WordCategory.nomen:\n",
    "            res[syn] = relatedness_calculator.simple_path(syn, syn_p)\n",
    "    if(len(res)>0):\n",
    "        return max(res, key=res.get)\n",
    "    else:\n",
    "        return \"no nouns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_related_to_politician_ic(list_syns):\n",
    "    syn_p = germanet.get_synsets_by_orthform(\"Politiker\").pop()\n",
    "    res = {}\n",
    "    for syn in list_syns:\n",
    "        if syn.word_category == WordCategory.nomen:\n",
    "            res[syn] = relatedness_nouns.resnik(syn, syn_p, normalize=True)\n",
    "    if(len(res)>0):\n",
    "        return max(res, key=res.get)\n",
    "    else:\n",
    "        return \"no nouns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['best_syn'], dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-10ed1a99a639>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_bonus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"best_syn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_bonus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfind_most_related_to_politician_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"synsets_ger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_bonus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"best_syn_ic\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_bonus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfind_most_related_to_politician_ic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"synsets_ger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m         self._validate_read_indexer(\n\u001b[0m\u001b[1;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['best_syn'], dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "data_bonus[\"best_syn\"] = data_bonus.apply(lambda row: find_most_related_to_politician_path(row[\"synsets_ger\"]) ,axis=1)\n",
    "data_bonus[\"best_syn_ic\"] = data_bonus.apply(lambda row: find_most_related_to_politician_ic(row[\"synsets_ger\"]) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suggestion_ger</th>\n",
       "      <th>synsets_ger</th>\n",
       "      <th>lexunits</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>lexunits_hypernyms</th>\n",
       "      <th>hyponyms</th>\n",
       "      <th>lexunits_hyponyms</th>\n",
       "      <th>city</th>\n",
       "      <th>synsets_check</th>\n",
       "      <th>best_syn</th>\n",
       "      <th>best_syn_ic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>[Synset(id=s26358, lexunits=Kot, Scheiße, Exkr...</td>\n",
       "      <td>[Kot, Scheiße, Exkrement, Stuhl, Kacke, Kaka, ...</td>\n",
       "      <td>{Synset(id=s26356, lexunits=Fäkalien)}</td>\n",
       "      <td>[Fäkalien]</td>\n",
       "      <td>{Synset(id=s26359, lexunits=Mist, Dung), Synse...</td>\n",
       "      <td>[Mist, Dung, Klabusterbeere, Kinderkacke, Losu...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Synset(id=s26358, lexunits=Kot, Scheiße, Exkre...</td>\n",
       "      <td>Synset(id=s26358, lexunits=Kot, Scheiße, Exkre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abbruch</td>\n",
       "      <td>[Synset(id=s106285, lexunits=Abbruch), Synset(...</td>\n",
       "      <td>[Abbruch, Abbruch, Beendigung, Beenden, Aufhör...</td>\n",
       "      <td>{Synset(id=s17129, lexunits=Schaden, Schädigung)}</td>\n",
       "      <td>[Schaden, Schädigung]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>Synset(id=s106337, lexunits=Abbruch)</td>\n",
       "      <td>Synset(id=s106337, lexunits=Abbruch)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>abnehmen</td>\n",
       "      <td>[Synset(id=s52508, lexunits=wegnehmen, abnehme...</td>\n",
       "      <td>[wegnehmen, abnehmen, fortnehmen, abchecken, a...</td>\n",
       "      <td>{Synset(id=s52497, lexunits=nehmen)}</td>\n",
       "      <td>[nehmen]</td>\n",
       "      <td>{Synset(id=s97237, lexunits=rupfen), Synset(id...</td>\n",
       "      <td>[rupfen, entrechten, entziehen, abknöpfen, ent...</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>no nouns</td>\n",
       "      <td>no nouns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>abschied</td>\n",
       "      <td>[Synset(id=s17481, lexunits=Abschied, Lebewohl...</td>\n",
       "      <td>[Abschied, Lebewohl, Verabschiedung, Abschied]</td>\n",
       "      <td>{Synset(id=s17478, lexunits=Trennung)}</td>\n",
       "      <td>[Trennung]</td>\n",
       "      <td>{Synset(id=s123668, lexunits=Bühnenabschied)}</td>\n",
       "      <td>[Bühnenabschied]</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Synset(id=s105682, lexunits=Abschied)</td>\n",
       "      <td>Synset(id=s17481, lexunits=Abschied, Lebewohl,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>adel</td>\n",
       "      <td>[Synset(id=s32247, lexunits=Adelstitel, Adel, ...</td>\n",
       "      <td>[Adelstitel, Adel, Adelsbezeichnung, Adelsgesc...</td>\n",
       "      <td>{Synset(id=s32215, lexunits=Titel)}</td>\n",
       "      <td>[Titel]</td>\n",
       "      <td>{Synset(id=s61467, lexunits=Kaiserinmutter), S...</td>\n",
       "      <td>[Kaiserinmutter, Marchese, Doge, Raugraf, Baro...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Synset(id=s32247, lexunits=Adelstitel, Adel, A...</td>\n",
       "      <td>Synset(id=s24212, lexunits=Adelsgeschlecht, Adel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>zirkus</td>\n",
       "      <td>[Synset(id=s108415, lexunits=Zirkus), Synset(i...</td>\n",
       "      <td>[Circus, Zirkus, Circus, Zirkus, Zirkusunterne...</td>\n",
       "      <td>{Synset(id=s42745, lexunits=Arena, Stadion)}</td>\n",
       "      <td>[Arena, Stadion]</td>\n",
       "      <td>{}</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Synset(id=s108415, lexunits=Zirkus)</td>\n",
       "      <td>Synset(id=s108415, lexunits=Zirkus)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>zitat</td>\n",
       "      <td>[Synset(id=s32274, lexunits=Zitat), Synset(id=...</td>\n",
       "      <td>[Zitat, Zitat]</td>\n",
       "      <td>{Synset(id=s32267, lexunits=Redewendung, Idiom...</td>\n",
       "      <td>[Redewendung, Idiom, Redensart, Wendung]</td>\n",
       "      <td>{Synset(id=s137738, lexunits=Selbstzitat)}</td>\n",
       "      <td>[Selbstzitat]</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Synset(id=s32501, lexunits=Zitat)</td>\n",
       "      <td>Synset(id=s32274, lexunits=Zitat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>zug</td>\n",
       "      <td>[Synset(id=s76189, lexunits=Zug), Synset(id=s4...</td>\n",
       "      <td>[Zug, Luftzug, Zug, Luft, Gesichtszug, Zug, Zu...</td>\n",
       "      <td>{Synset(id=s24150, lexunits=Formation, Gruppie...</td>\n",
       "      <td>[Formation, Gruppierung]</td>\n",
       "      <td>{Synset(id=s133640, lexunits=Einsatzzug), Syns...</td>\n",
       "      <td>[Einsatzzug, Fernmeldezug]</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>Synset(id=s109599, lexunits=Zug)</td>\n",
       "      <td>Synset(id=s76189, lexunits=Zug)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>zukunft</td>\n",
       "      <td>[Synset(id=s51054, lexunits=Zukunft, Vorzeitig...</td>\n",
       "      <td>[Zukunft, Vorzeitigkeit, Hinkunft, Futur, Zuku...</td>\n",
       "      <td>{Synset(id=s51051, lexunits=Zeitstufe)}</td>\n",
       "      <td>[Zeitstufe]</td>\n",
       "      <td>{Synset(id=s22920, lexunits=Nachwelt), Synset(...</td>\n",
       "      <td>[Nachwelt, Vorweg, Energiezukunft]</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Synset(id=s51054, lexunits=Zukunft, Vorzeitigk...</td>\n",
       "      <td>Synset(id=s51054, lexunits=Zukunft, Vorzeitigk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>zusammenbruch</td>\n",
       "      <td>[Synset(id=s17538, lexunits=Kollaps, Zusammenb...</td>\n",
       "      <td>[Kollaps, Zusammenbruch, Zusammenbruch, Crash]</td>\n",
       "      <td>{Synset(id=s17096, lexunits=Vorfall, Begebenhe...</td>\n",
       "      <td>[Vorfall, Begebenheit, Begebnis, Vorkommnis]</td>\n",
       "      <td>{Synset(id=s17539, lexunits=Verkehrskollaps)}</td>\n",
       "      <td>[Verkehrskollaps]</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Synset(id=s17538, lexunits=Kollaps, Zusammenbr...</td>\n",
       "      <td>Synset(id=s17538, lexunits=Kollaps, Zusammenbr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>451 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     suggestion_ger                                        synsets_ger  \\\n",
       "0                aa  [Synset(id=s26358, lexunits=Kot, Scheiße, Exkr...   \n",
       "6           abbruch  [Synset(id=s106285, lexunits=Abbruch), Synset(...   \n",
       "15         abnehmen  [Synset(id=s52508, lexunits=wegnehmen, abnehme...   \n",
       "18         abschied  [Synset(id=s17481, lexunits=Abschied, Lebewohl...   \n",
       "26             adel  [Synset(id=s32247, lexunits=Adelstitel, Adel, ...   \n",
       "...             ...                                                ...   \n",
       "3741         zirkus  [Synset(id=s108415, lexunits=Zirkus), Synset(i...   \n",
       "3742          zitat  [Synset(id=s32274, lexunits=Zitat), Synset(id=...   \n",
       "3752            zug  [Synset(id=s76189, lexunits=Zug), Synset(id=s4...   \n",
       "3753        zukunft  [Synset(id=s51054, lexunits=Zukunft, Vorzeitig...   \n",
       "3755  zusammenbruch  [Synset(id=s17538, lexunits=Kollaps, Zusammenb...   \n",
       "\n",
       "                                               lexunits  \\\n",
       "0     [Kot, Scheiße, Exkrement, Stuhl, Kacke, Kaka, ...   \n",
       "6     [Abbruch, Abbruch, Beendigung, Beenden, Aufhör...   \n",
       "15    [wegnehmen, abnehmen, fortnehmen, abchecken, a...   \n",
       "18       [Abschied, Lebewohl, Verabschiedung, Abschied]   \n",
       "26    [Adelstitel, Adel, Adelsbezeichnung, Adelsgesc...   \n",
       "...                                                 ...   \n",
       "3741  [Circus, Zirkus, Circus, Zirkus, Zirkusunterne...   \n",
       "3742                                     [Zitat, Zitat]   \n",
       "3752  [Zug, Luftzug, Zug, Luft, Gesichtszug, Zug, Zu...   \n",
       "3753  [Zukunft, Vorzeitigkeit, Hinkunft, Futur, Zuku...   \n",
       "3755     [Kollaps, Zusammenbruch, Zusammenbruch, Crash]   \n",
       "\n",
       "                                              hypernyms  \\\n",
       "0                {Synset(id=s26356, lexunits=Fäkalien)}   \n",
       "6     {Synset(id=s17129, lexunits=Schaden, Schädigung)}   \n",
       "15                 {Synset(id=s52497, lexunits=nehmen)}   \n",
       "18               {Synset(id=s17478, lexunits=Trennung)}   \n",
       "26                  {Synset(id=s32215, lexunits=Titel)}   \n",
       "...                                                 ...   \n",
       "3741       {Synset(id=s42745, lexunits=Arena, Stadion)}   \n",
       "3742  {Synset(id=s32267, lexunits=Redewendung, Idiom...   \n",
       "3752  {Synset(id=s24150, lexunits=Formation, Gruppie...   \n",
       "3753            {Synset(id=s51051, lexunits=Zeitstufe)}   \n",
       "3755  {Synset(id=s17096, lexunits=Vorfall, Begebenhe...   \n",
       "\n",
       "                                lexunits_hypernyms  \\\n",
       "0                                       [Fäkalien]   \n",
       "6                            [Schaden, Schädigung]   \n",
       "15                                        [nehmen]   \n",
       "18                                      [Trennung]   \n",
       "26                                         [Titel]   \n",
       "...                                            ...   \n",
       "3741                              [Arena, Stadion]   \n",
       "3742      [Redewendung, Idiom, Redensart, Wendung]   \n",
       "3752                      [Formation, Gruppierung]   \n",
       "3753                                   [Zeitstufe]   \n",
       "3755  [Vorfall, Begebenheit, Begebnis, Vorkommnis]   \n",
       "\n",
       "                                               hyponyms  \\\n",
       "0     {Synset(id=s26359, lexunits=Mist, Dung), Synse...   \n",
       "6                                                    {}   \n",
       "15    {Synset(id=s97237, lexunits=rupfen), Synset(id...   \n",
       "18        {Synset(id=s123668, lexunits=Bühnenabschied)}   \n",
       "26    {Synset(id=s61467, lexunits=Kaiserinmutter), S...   \n",
       "...                                                 ...   \n",
       "3741                                                 {}   \n",
       "3742         {Synset(id=s137738, lexunits=Selbstzitat)}   \n",
       "3752  {Synset(id=s133640, lexunits=Einsatzzug), Syns...   \n",
       "3753  {Synset(id=s22920, lexunits=Nachwelt), Synset(...   \n",
       "3755      {Synset(id=s17539, lexunits=Verkehrskollaps)}   \n",
       "\n",
       "                                      lexunits_hyponyms   city  synsets_check  \\\n",
       "0     [Mist, Dung, Klabusterbeere, Kinderkacke, Losu...  False              2   \n",
       "6                                                    []  False              5   \n",
       "15    [rupfen, entrechten, entziehen, abknöpfen, ent...  False             10   \n",
       "18                                     [Bühnenabschied]  False              2   \n",
       "26    [Kaiserinmutter, Marchese, Doge, Raugraf, Baro...  False              2   \n",
       "...                                                 ...    ...            ...   \n",
       "3741                                                 []  False              4   \n",
       "3742                                      [Selbstzitat]  False              2   \n",
       "3752                         [Einsatzzug, Fernmeldezug]  False             14   \n",
       "3753                 [Nachwelt, Vorweg, Energiezukunft]  False              2   \n",
       "3755                                  [Verkehrskollaps]  False              2   \n",
       "\n",
       "                                               best_syn  \\\n",
       "0     Synset(id=s26358, lexunits=Kot, Scheiße, Exkre...   \n",
       "6                  Synset(id=s106337, lexunits=Abbruch)   \n",
       "15                                             no nouns   \n",
       "18                Synset(id=s105682, lexunits=Abschied)   \n",
       "26    Synset(id=s32247, lexunits=Adelstitel, Adel, A...   \n",
       "...                                                 ...   \n",
       "3741                Synset(id=s108415, lexunits=Zirkus)   \n",
       "3742                  Synset(id=s32501, lexunits=Zitat)   \n",
       "3752                   Synset(id=s109599, lexunits=Zug)   \n",
       "3753  Synset(id=s51054, lexunits=Zukunft, Vorzeitigk...   \n",
       "3755  Synset(id=s17538, lexunits=Kollaps, Zusammenbr...   \n",
       "\n",
       "                                            best_syn_ic  \n",
       "0     Synset(id=s26358, lexunits=Kot, Scheiße, Exkre...  \n",
       "6                  Synset(id=s106337, lexunits=Abbruch)  \n",
       "15                                             no nouns  \n",
       "18    Synset(id=s17481, lexunits=Abschied, Lebewohl,...  \n",
       "26    Synset(id=s24212, lexunits=Adelsgeschlecht, Adel)  \n",
       "...                                                 ...  \n",
       "3741                Synset(id=s108415, lexunits=Zirkus)  \n",
       "3742                  Synset(id=s32274, lexunits=Zitat)  \n",
       "3752                    Synset(id=s76189, lexunits=Zug)  \n",
       "3753  Synset(id=s51054, lexunits=Zukunft, Vorzeitigk...  \n",
       "3755  Synset(id=s17538, lexunits=Kollaps, Zusammenbr...  \n",
       "\n",
       "[451 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bonus.to_excel(\"data.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
